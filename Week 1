WEEK 1 – Literature survey as completed in phase-1 

The first week of the project was dedicated to developing a deep and comprehensive understanding of the research background related to sign language translation systems, their evolution, technological foundations, and limitations in current solutions. This literature survey revision formed the intellectual foundation of the entire project. Instead of immediately beginning development, Week 1 focused on building theoretical clarity and identifying research gaps by analyzing existing works, published studies, datasets, algorithms, and translation techniques. By revisiting, refining, and expanding the literature survey, the team ensured that the direction of the project was rooted in academic understanding and not random experimentation.
Sign language recognition has been an active area of research for more than two decades, evolving from early sensor-based systems—like glove-based tracking—to modern AI-driven models that rely on computer vision, deep learning, and natural language processing. The literature examined during Week 1 highlighted how earlier approaches faced significant constraints due to limited hardware capabilities and the absence of large, standardized sign language datasets. However, with the emergence of powerful GPUs, frameworks such as TensorFlow and PyTorch, and platforms like MediaPipe, real-time translation has become more feasible. This week allowed the team to analyze these technological transformations in detail.
One of the key research papers reviewed during this revision week was Papatsimouli et al. (2022), which presented a comprehensive overview of real-time sign language translation systems. The authors highlighted the challenges of capturing complex hand movements, facial expressions, and body pose variations. The paper emphasized that real-time accuracy in translation depends heavily on high-quality datasets and robust deep learning architectures. This study helped the team understand the importance of choosing the correct input representation—whether pixel-based, keypoint-based, or mixed modalities. The authors also noted that, despite advanced detection tools like MediaPipe, achieving real-world accuracy remains complicated due to environmental factors like lighting, camera angle, and background noise. This insight encouraged us to incorporate preprocessing and data normalization strategies in our proposed tool.
Another important work examined was Liang, Li, and Chai (2023), who conducted a detailed survey on techniques used in sign language translation, including CNNs, RNNs, LSTMs, and Transformers. Their work highlighted a fundamental challenge: sign languages are not simple gesture sequences but complete languages with grammar, syntax, and context. Therefore, successful translation requires more than gesture classification; it demands linguistic understanding. The authors discussed how modern SLT systems integrate Natural Language Processing with Computer Vision to convert sequences of gestures into meaningful sentences. This perspective guided our team to consider integrating NLP-based post-processing in later stages of the project to enhance translation accuracy.
The Week 1 revision also included a study of Shinde et al. (2023), whose research focused on extracting hand landmark keypoints from images using computer vision techniques. The paper demonstrated how gesture recognition accuracy improved significantly when models were trained on numerical representations of hand skeletons rather than raw image pixels. This reinforced our decision to use MediaPipe Hands for keypoint detection. The researchers also noted that this approach minimizes computational cost, making real-time web-based deployment more practical.
Another paper, authored by Sujay et al. (2022), discussed the implementation of sign language translation using the MediaPipe module and Raspberry Pi. Although their work targeted embedded devices rather than web browsers, it demonstrated the flexibility and power of MediaPipe as a real-time pose and hand recognition framework. The study proved that lightweight solutions can still achieve high accuracy, especially when combined with simple machine learning models. This was an important reference point for our project because we aimed to create a lightweight web-based solution without depending on heavy computational resources.
